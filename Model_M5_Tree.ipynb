{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c504104-b540-40df-a8ee-91dfe91b6fcd",
   "metadata": {},
   "source": [
    "<div>\n",
    "<table style=\"width: 100%\">\n",
    "    <td>\n",
    "\t<tr>\n",
    "\t\t<td>\n",
    "\t\t<table style=\"width: 100%\">\n",
    "\t\t\t<tr>\n",
    "                <td ><center><font size=\"30\">WSD 2024-2025</font><center>\n",
    "                    <center><font size=\"30\">AI 4 Water Systems</font><center></td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "                <td><center><font size=\"5\">Jupyter Notebook 2</font><center></td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "                <td><center><font size=\"10\">Model M5 Tree</font><center></td>\n",
    "\t\t\t</tr>\n",
    "            <tr>\n",
    "                <td><center><font size=\"5\">Claudia Bertini, Lecturer in Hydroinformatics</font><center></td>\n",
    "\t\t\t</tr> \n",
    "\t\t</table>\n",
    "\t\t<td> <img src='ihe-logo-square.png'></img></td>\n",
    "\t</tr>\n",
    "</table>\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395b6c7-a76f-42e8-889f-89facdf93021",
   "metadata": {},
   "source": [
    "# Recap and scope of this notebook\n",
    "We aim at developing a data driven model that is predicting discharge in the next hour (Qt+1) using past rainfall and discharge hourly data. Throughout these 3 notebooks, we will learn how to pre-define the input features of the data driven model (Notebook 1), how to build a linear regression model and a M5 Tree model (Notebook 2), how to build an Artificial Neural Network (ANN, Notebook 3).\n",
    "\n",
    "In <b>Notebook 1</b> we learnt how to load and visualize the data. We also used (linear) correlation and autocorrelation to identify the potential relevant input features. Based on our results, we concluded that potentially we could use the past 6 hours of both effective rainfall and discharge to train our data driven model. Usually, these inputs can be further refined, by trial and error: one defines all possible combinations of the input variables (that make sense) and train and test a data driven model using them. Let's assume that we have 100 combinations of input features, which results in 100 different data driven models trained. We then test all the 100 models on the same testing dataset and we compute one or more performance metrics (RMSE, MSE, NSE, etc.). The model(s) with best performance is (are) the one with the best set of input features. This process can take quite some time and for the purpose of this exercise, we already did it. Our results indicate that the <b>most relevant input features</b> to be used to predict Qt+1 are:\n",
    "<b>REt, REt-1, REt-2, REt-3, REt-4, REt-5, Qt, Qt-1, Qt-2.</b> \n",
    "\n",
    "In this Notebook 2, we will use these features to build first a <b>Linear regression model</b> and then a <b>M5 Tree model</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966a886-1946-4d19-b965-216c5b0f2ed6",
   "metadata": {},
   "source": [
    "# 1. Installing and importing the libraries needed\n",
    "Before importing the libraries below, you might need to install them. You can use the following commands (the code to be run is in the following cells):\n",
    "pip install matplotlib\n",
    "pip install pandas\n",
    "pip install openpyxl\n",
    "\n",
    "The library os should be automatically installed. If it is not the case, you can add a cell and type \"pip install os\", then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667e1a7-7ba2-41df-9288-b576c578bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237010c9-9e87-47d3-a713-857de381c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20016dea-e1a4-4b4e-b629-8e2135b14e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1b559-2d6d-4967-8099-8785f25e7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install m5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63063e6-a2c7-4868-bf41-26770dd35ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499cba5e-c648-4995-b2d2-bde21e81ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50062f60-7ed4-4ce6-8750-17dde578ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from m5py import M5Prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e3bfd-668e-4456-9076-2ab455159145",
   "metadata": {},
   "source": [
    "# 2. Load the data\n",
    "To load the data correctly, first make sure that the file \"Sieve-orig.xlsx\" is saved in the same folder of this notebook. If it's not the case, please move the file to the folder of this notebook.\n",
    "The file contains the hourly records of discharge (Qt) and effective precipitation (REt), from 1 Dec 1969 to 28 Feb 1970. There is also one column providing information about date and time.\n",
    "In the first lines of the next cell, you will find information to be able to open this notebook in Google Colab, in case you prefer it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7c9bc-853f-4a30-9dbf-eb6a41153fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path and check environment\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    base_path = '/content/drive/My Drive/'\n",
    "else:\n",
    "    base_path = os.getcwd()  # Use current directory in Jupyter Notebook = directory where the notebook is locally saved in your computer\n",
    "\n",
    "# Ensure file exists before reading\n",
    "file_name = 'Sieve-orig.xlsx'\n",
    "file_path = os.path.join(base_path, file_name)\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "# First load the file with the data from Sieve\n",
    "df = pd.read_excel(file_path)\n",
    "# you can visualize the first rows of the data by:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422586e2-537c-4c53-97e9-2484d6113528",
   "metadata": {},
   "source": [
    "# 3. Select the input features\n",
    "Using the information on which input feature to retain contained at the beginning of this notebook, we now re-organize the dataset. We want each variable to be in one specific column. We can use (part) of the code we developed for the correlation analysis for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7050c26d-134e-4420-b8b3-4ae9e13c4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first shift our rainfall data and copy them to separate columns. we take up to 5 steps back in time\n",
    "for lag in range(1, 6):  # 10 steps back\n",
    "    df[f'REt_lag{lag}'] = df['REt'].shift(lag)\n",
    "    \n",
    "# we do the same with discharge, taking up to 2 steps back in time\n",
    "for lag in range(1, 3):  # 10 steps back\n",
    "    df[f'Qt_lag{lag}'] = df['Qt'].shift(lag)\n",
    "\n",
    "# you can print the headers if you want to visualize your data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a4589-6536-4922-9cd3-915a44916e3c",
   "metadata": {},
   "source": [
    "You see now several rows with NaN (not a number) values. It is normal, as we are shifting the rows back. We can remove the NaN rows in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a1f8a-911d-4371-9db1-8714f90c162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create the target (Qt+1) column, which was not yet in the df\n",
    "df['Qt+1'] = df['Qt'].shift(-1)\n",
    "\n",
    "# now we remove the row with missing values (NaN = not a number)\n",
    "df = df.dropna()\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# you can print the headers if you want to visualize your data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481398b-321c-443d-844f-7165853dee16",
   "metadata": {},
   "source": [
    "You can now see that all the variables have a dedicated column and that there are no more NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6772ff0-8af9-4f9d-878a-ee978236c9d0",
   "metadata": {},
   "source": [
    "# 4. Training-Testing split\n",
    "We now have to split the dataset into two parts, one used for training and one for testing purposes. For the testing part, we use the first 300 rows of our dataset, while we keep the rows from 301 to the very end for training.\n",
    "It is also possible to use a an automatic training-testing split function built in scikit learn library, but it would not allow us to choose the period.\n",
    "\n",
    "We then prepare the Input (X) and output (Y) datasets for our data driven model, being very careful that the target Qt+1 is not included in the Input (X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb166e-768c-47b2-93ba-0d25d837fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into training and testing, taking the first 300 rows for testing,\n",
    "# and the rows from 301 onwards for training.\n",
    "df_test = df.loc[:299]\n",
    "df_train = df.loc[300:]\n",
    "\n",
    "# we can visualize them\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3c3f2-6402-4638-bab7-ae7c5950a886",
   "metadata": {},
   "source": [
    "We can see that the inputs and the output (Qt+1) are still in the same dataset, so we need to split them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5bc99-092e-449e-b36d-befd4a65f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we prepare the inputs (X) and the target (Y), being sure that X does not contain\n",
    "# the target (Qt+1) and that Y contains only the target (Qt+1)\n",
    "\n",
    "X_train = df_train.copy().drop(['Date','Qt+1'],axis=1)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9218b-3b9b-46e6-9c80-16cf7cec6847",
   "metadata": {},
   "source": [
    "We can now see that the Qt+1 column is no longer in the dataframe. We repeat the same for the testing set and for the outputs. We also transform all X and Y into numpy array, as requested by the linear regression model that we will implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9f5df-e4f1-49e7-b07f-a9e7cbeeb272",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_test = df_test.copy().drop(['Date','Qt+1'],axis=1).to_numpy()\n",
    "\n",
    "y_train = df_train['Qt+1'].to_numpy()\n",
    "y_test = df_test['Qt+1'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1061df-dcfb-416c-88f2-66e6846134ab",
   "metadata": {},
   "source": [
    "# 5. Training and Testing the Linear Regression Model\n",
    "We will now build and train a simple linear regression model. This kind of model is usually employed in studies with data driven models and Machine Learning as a benchmark against more fancy and complicated models, to see whether a more complicated model is needed or a simple linear regression yields good results already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec257e-03ed-47af-af74-5337af895ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multi-Linear Regression Model\n",
    "mlr_model = LinearRegression()  # this just calls the model\n",
    "mlr_model.fit(X_train, y_train) # this line actually TRAIN the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715030cb-0a0b-4cb5-b283-8a3776e2a8da",
   "metadata": {},
   "source": [
    "Now we use the fitted model to predict the Qt+1 on the test set and we compute some metrics. The sklearn library does not have a built in method to track the loss function values during training. You can see it by applying the model on the training data and then computing the metrics. We show only the case for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787d88f-f41d-4660-91db-c098347a15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = mlr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Multi-Linear Regression Model Performance:')\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d6b61-60a6-4315-9ed2-16d6da341ef4",
   "metadata": {},
   "source": [
    "There are several metrics that you can compute. You can look into the sklearn documentation which ones are already built in for you to use.\n",
    "\n",
    "We can now plot the results, to see how they compare graphically to our observations. You can plot them in a scatterplot or in a regular plot. If you use the scatterplot, the data should align along the bisect to have a perfect simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e10ee-e8b6-4753-a2be-0bdce851d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted discharge\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test, y_pred, color='darkolivegreen', alpha=0.6, label='Predicted vs Actual')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='black', linestyle='--', label='Perfect Fit')\n",
    "plt.xlabel('Actual Discharge (m³/s)')\n",
    "plt.ylabel('Predicted Discharge (m³/s)')\n",
    "plt.title('Multi-Linear Regression: Predicted vs Actual Discharge')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "scatter_path = os.path.join(base_path, 'LinearModel_Scatter.png')\n",
    "# save the plot on your local folder\n",
    "plt.savefig(scatter_path, dpi=600, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03867c20-ccb5-4a51-baff-578a8c0171ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hydrographs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test, label='Actual Discharge', color='navy')\n",
    "plt.plot(y_pred, label='Predicted Discharge', color='steelblue', linestyle='dashed')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Discharge (m³/s)')\n",
    "plt.title('Actual vs Predicted Discharge Hydrograph')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "hydro_path = os.path.join(base_path, 'LinearModel_Hydrograph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad618f-dbcc-4c2a-b9dc-d8e27e29df20",
   "metadata": {},
   "source": [
    "# 6. Training and Testing the M5 tree model\n",
    "We now train the M5 tree model, using the same procedure as before. We do not need to change the input features and target, as they are always the same of the Linear model.\n",
    "\n",
    "We will first train an <b>unpruned</b> M5 tree, then we will train a <b>pruned</b> model and check the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0275b0-9e29-4146-8cea-803deb704bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with an unpruned model\n",
    "m5_unpruned = M5Prime(use_pruning=False) # call the model\n",
    "m5_unpruned.fit(X_train, y_train) # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe76a6b-9e62-44cf-81f0-d4252b95c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_unpruned = m5_unpruned.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce5902-6525-4063-9afe-01129166238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_unpruned)\n",
    "print(f'Mean Squared Error: {mse:.2f} (m³/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e441f-c83d-4ca5-ab76-74f38dfc718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can print the trees\n",
    "regr_1_label = 'unpruned'\n",
    "print(\"\\n----- %s\" % regr_1_label)\n",
    "print(m5_unpruned.as_pretty_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a32ae7-0016-4ff6-943d-40e7e73ef58f",
   "metadata": {},
   "source": [
    "Now we repeat it with the pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0a3f8-75c2-4115-acc1-74e992d31722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now repeat with a pruned model\n",
    "m5_pruned = M5Prime(use_pruning=True) # call the model\n",
    "m5_pruned.fit(X_train, y_train) # train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b6d2c-79f6-4b5e-8725-ba99b1dcab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pruned = m5_pruned.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1237c-b4b3-4445-8c13-71ba3d3357fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pruned)\n",
    "print(f'Mean Squared Error: {mse:.2f} (m³/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6baad-2148-4bfe-925e-66eb6d689c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can print the trees\n",
    "regr_2_label = 'pruned'\n",
    "print(\"\\n----- %s\" % regr_2_label)\n",
    "print(m5_pruned.as_pretty_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8ae76-03f0-4a0b-8b91-be78ca157a42",
   "metadata": {},
   "source": [
    "<b> Which model is the best? Do you see much difference between the pruned and the unpruned versions? </b>\n",
    "\n",
    "You can keep exploring different training strategies, such as for instance defining a minimum number of leaves per node and/or a maximum tree depth. Both options can be used together with the pruning. You can see an example below.\n",
    "\n",
    "<b>?</b>  You can reduce the number of min_samples_leaf and increase at the same time the max_depth. Look at the MSE: how does it change? Is the model overfitting or underfitting?  Repeat the same by increasing the number of min_samples_leaf and reducing at the same time the max_depth. What changes? Are you now in underfitting or overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da879a6b-b586-48fd-90c8-6881886b5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m5_leaves = M5Prime(min_samples_leaf=4,max_depth=15)\n",
    "# note that in the line above you have just called the model, you have not fitted it\n",
    "# on the training data yet. Remember to call model.fit(X_train, y_train) to train the model.\n",
    "m5_leaves.fit(X_train,y_train)\n",
    "y_leaves = m5_leaves.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_leaves)\n",
    "print(f'Mean Squared Error: {mse:.2f} (m³/s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42cb34-79dc-4860-b818-93d8480fe57d",
   "metadata": {},
   "source": [
    "We can now plot all the model predictions together, and check (also) graphically which works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e720fb-97b3-4ca7-9fec-c9bc2f001918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can merge the predictions in one pandas dataframe, and then loop through it to plot\n",
    "predictions = ['Linear', 'M5-unpruned', 'M5-pruned', 'M5-leaves/depth']\n",
    "dr = pd.DataFrame(columns=predictions)\n",
    "dr['Linear']= pd.DataFrame(y_pred)\n",
    "dr['M5-unpruned']= pd.DataFrame(y_unpruned)\n",
    "dr['M5-pruned']= pd.DataFrame(y_pruned)\n",
    "dr['M5-leaves/depth']= pd.DataFrame(y_leaves)\n",
    "dr['Date'] = pd.to_datetime(df['Date'].loc[:299])\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(4, 1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "predictions = ['Linear', 'M5-unpruned', 'M5-pruned', 'M5-leaves/depth']\n",
    "labels = ['Linear', 'M5-unpruned', 'M5-pruned', 'M5-leaves/depth']\n",
    "colors = ['steelblue', 'firebrick', 'olivedrab', 'orchid']\n",
    "legend_handles = []\n",
    "for i, ax in enumerate(axes):\n",
    "    actual, = ax.plot( dr['Date'],y_test, label='Actual Discharge', color='navy',linestyle='dashed')\n",
    "    predicted, = ax.plot(dr['Date'],dr[predictions[i]], label=labels[i], color=colors[i],alpha=0.7)\n",
    "    ax.set_ylabel('Discharge (m³/s)')\n",
    "    ax.grid(True)\n",
    "    # Collect handles only from the first subplot\n",
    "    if i == 0:\n",
    "        legend_handles.append(actual)\n",
    "    legend_handles.append(predicted)\n",
    "\n",
    "# Global legend\n",
    "fig.legend(legend_handles, ['Actual Discharge'] + labels, loc='upper center', \n",
    "           fontsize=10, ncol=5, bbox_to_anchor=(0.5, 0.95))\n",
    "\n",
    "plt.suptitle('Comparison of Actual vs Predicted Discharge')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit global title and legend\n",
    "plt.show()\n",
    "subplot_path = os.path.join(base_path, 'LinearvsM5_Hydrographs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2717e68-44a7-4ee8-8a8b-706d1df5c796",
   "metadata": {},
   "source": [
    "<b> We can now save our results locally, in our laptop </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32154d8-c50d-4cf2-a7b0-25ab063b21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we also add the observations to the dataframe dr with the results\n",
    "dr['Obs'] = pd.DataFrame(y_test)\n",
    "dr.head()\n",
    "\n",
    "# Now we save them\n",
    "results_path = os.path.join(base_path, 'M5_Linear_Predictions.xlsx')\n",
    "dr.to_excel(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b3461-627f-4075-84d4-4ab12466bac5",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "We have learnt how to split the training and testing data using pandas library. We then learnt how to call, train and test a simple Linear regression model and a M5 tree model. We have explored different training options for the M5 tree. Finally, we have compared the performances of all the models using the mean squared error and with graphical inspections. What can you conclude about the different models tested? Which would you choose as the best model?\n",
    "\n",
    "You can further explore the different modelling options by reducing the number of input features used by the models. For a fair comparison across models, use the same input features in all models. What changes in the model performance? Is the reduction of input features worth the increase/decrease in MSE? Which set of input feautures would you recommend?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
