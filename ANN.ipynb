{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c504104-b540-40df-a8ee-91dfe91b6fcd",
   "metadata": {},
   "source": [
    "<div>\n",
    "<table style=\"width: 100%\">\n",
    "    <td>\n",
    "\t<tr>\n",
    "\t\t<td>\n",
    "\t\t<table style=\"width: 100%\">\n",
    "\t\t\t<tr>\n",
    "                <td ><center><font size=\"30\">WSD 2024-2025</font><center>\n",
    "                    <center><font size=\"30\">AI 4 Water Systems</font><center></td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "                <td><center><font size=\"5\">Jupyter Notebook 2</font><center></td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "                <td><center><font size=\"10\">Model M5 Tree</font><center></td>\n",
    "\t\t\t</tr>\n",
    "            <tr>\n",
    "                <td><center><font size=\"5\">Claudia Bertini, Lecturer in Hydroinformatics</font><center></td>\n",
    "\t\t\t</tr> \n",
    "\t\t</table>\n",
    "\t\t<td> <img src='ihe-logo-square.png'></img></td>\n",
    "\t</tr>\n",
    "</table>\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395b6c7-a76f-42e8-889f-89facdf93021",
   "metadata": {},
   "source": [
    "# Recap and scope of this notebook\n",
    "We aim at developing a data driven model that is predicting discharge in the next hour (Qt+1) using past rainfall and discharge hourly data. Throughout these 3 notebooks, we will learn how to pre-define the input features of the data driven model (Notebook 1), how to build a linear regression model and a M5 Tree model (Notebook 2), how to build an Artificial Neural Network (ANN, Notebook 3).\n",
    "\n",
    "In <b>Notebook 1</b> we learnt how to load and visualize the data. We also used (linear) correlation and autocorrelation to identify the potential relevant input features. Based on our results, we concluded that potentially we could use the past 6 hours of both effective rainfall and discharge to train our data driven model. In Notebook 2, we learnt how to use these features to build first a <b>Linear regression model</b> and then a <b>M5 Tree model</b>. We explored different training options for the M5 model: pruned, unpruned, and with custom made constraints on the number of minimum leaves allowed in a node and maximum depth of the tree.\n",
    "In this <b> Notebook 3 </b>, we will learn how to develop (train and test) an Artificial Neural Network (ANN) to predict hourly discharge (Qt+1) using the same input features as for the linear and M5 models. More specifically, we will learn how to develop a Multi Layer Perceptron (MLP), a specific kind of ANN. The first part of the notebook is almost identical to the initial part of Notebook 2, as the data loading and preparation (including training and testing split) is exactly the same. We repeat them here, so that you can run ANN and M5 indipendently one from another. Be careful in the initial libraries installed and loaded, because they are slightly different as we now import the library for the MLP and not the one for the Linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966a886-1946-4d19-b965-216c5b0f2ed6",
   "metadata": {},
   "source": [
    "# 1. Installing and importing the libraries needed\n",
    "Before importing the libraries below, you might need to install them. You can use the following commands (the code to be run is in the following cells):\n",
    "pip install matplotlib\n",
    "pip install pandas\n",
    "pip install openpyxl\n",
    "\n",
    "The library os should be automatically installed. If it is not the case, you can add a cell and type \"pip install os\", then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667e1a7-7ba2-41df-9288-b576c578bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237010c9-9e87-47d3-a713-857de381c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20016dea-e1a4-4b4e-b629-8e2135b14e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63063e6-a2c7-4868-bf41-26770dd35ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499cba5e-c648-4995-b2d2-bde21e81ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50062f60-7ed4-4ce6-8750-17dde578ade6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e3bfd-668e-4456-9076-2ab455159145",
   "metadata": {},
   "source": [
    "# 2. Load the data\n",
    "To load the data correctly, first make sure that the file \"Sieve-orig.xlsx\" is saved in the same folder of this notebook. If it's not the case, please move the file to the folder of this notebook.\n",
    "The file contains the hourly records of discharge (Qt) and effective precipitation (REt), from 1 Dec 1969 to 28 Feb 1970. There is also one column providing information about date and time.\n",
    "In the first lines of the next cell, you will find information to be able to open this notebook in Google Colab, in case you prefer it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7c9bc-853f-4a30-9dbf-eb6a41153fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define file path and check environment\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    base_path = '/content/drive/My Drive/'\n",
    "else:\n",
    "    base_path = os.getcwd()  # Use current directory in Jupyter Notebook = directory where the notebook is locally saved in your computer\n",
    "\n",
    "# Ensure file exists before reading\n",
    "file_name = 'Sieve-orig.xlsx'\n",
    "file_path = os.path.join(base_path, file_name)\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "# First load the file with the data from Sieve\n",
    "df = pd.read_excel(file_path)\n",
    "# you can visualize the first rows of the data by:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422586e2-537c-4c53-97e9-2484d6113528",
   "metadata": {},
   "source": [
    "# 3. Select the input features\n",
    "Using the information on which input feature to retain contained at the beginning of this notebook, we now re-organize the dataset. We want each variable to be in one specific column. We can use (part) of the code we developed for the correlation analysis for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7050c26d-134e-4420-b8b3-4ae9e13c4cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We first shift our rainfall data and copy them to separate columns. we take up to 5 steps back in time\n",
    "for lag in range(1, 6):  # 10 steps back\n",
    "    df[f'REt_lag{lag}'] = df['REt'].shift(lag)\n",
    "    \n",
    "# we do the same with discharge, taking up to 2 steps back in time\n",
    "for lag in range(1, 3):  # 10 steps back\n",
    "    df[f'Qt_lag{lag}'] = df['Qt'].shift(lag)\n",
    "\n",
    "# you can print the headers if you want to visualize your data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a4589-6536-4922-9cd3-915a44916e3c",
   "metadata": {},
   "source": [
    "You see now several rows with NaN (not a number) values. It is normal, as we are shifting the rows back. We can remove the NaN rows in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a1f8a-911d-4371-9db1-8714f90c162c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we create the target (Qt+1) column, which was not yet in the df\n",
    "df['Qt+1'] = df['Qt'].shift(-1)\n",
    "\n",
    "# now we remove the row with missing values (NaN = not a number)\n",
    "df = df.dropna()\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# you can print the headers if you want to visualize your data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481398b-321c-443d-844f-7165853dee16",
   "metadata": {},
   "source": [
    "You can now see that all the variables have a dedicated column and that there are no more NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6772ff0-8af9-4f9d-878a-ee978236c9d0",
   "metadata": {},
   "source": [
    "# 4. Training-Testing split\n",
    "We now have to split the dataset into two parts, one used for training and one for testing purposes. For the testing part, we use the first 300 rows of our dataset, while we keep the rows from 301 to the very end for training.\n",
    "It is also possible to use a an automatic training-testing split function built in scikit learn library, but it would not allow us to choose the period.\n",
    "\n",
    "We then prepare the Input (X) and output (Y) datasets for our data driven model, being very careful that the target Qt+1 is not included in the Input (X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb166e-768c-47b2-93ba-0d25d837fb43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We split the data into training and testing, taking the first 300 rows for testing,\n",
    "# and the rows from 301 onwards for training.\n",
    "df_test = df.loc[:299]\n",
    "df_train = df.loc[300:]\n",
    "\n",
    "# we can visualize them\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3c3f2-6402-4638-bab7-ae7c5950a886",
   "metadata": {},
   "source": [
    "We can see that the inputs and the output (Qt+1) are still in the same dataset, so we need to split them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5bc99-092e-449e-b36d-befd4a65f7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we prepare the inputs (X) and the target (Y), being sure that X does not contain\n",
    "# the target (Qt+1) and that Y contains only the target (Qt+1)\n",
    "\n",
    "X_train = df_train.copy().drop(['Date','Qt+1'],axis=1)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9218b-3b9b-46e6-9c80-16cf7cec6847",
   "metadata": {},
   "source": [
    "We can now see that the Qt+1 column is no longer in the dataframe. We repeat the same for the testing set and for the outputs. We also transform all X and Y into numpy array, as requested by the MLP model that we will implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9f5df-e4f1-49e7-b07f-a9e7cbeeb272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_test = df_test.copy().drop(['Date','Qt+1'],axis=1).to_numpy()\n",
    "\n",
    "y_train = df_train['Qt+1'].to_numpy()\n",
    "y_test = df_test['Qt+1'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b29914-7d69-4ac8-bad3-5b712dfee2e8",
   "metadata": {},
   "source": [
    "# 5. Data normalization\n",
    "Before training our MLP model, we first have to normalize the data. The normalization is desirable to make the training faster and easier, and make the algorithm converge more easily. Imagine, for instance, to have very different input variables, with different magnitude and ranges. It might happen, during training, that the input feature with the highest range (difference between max and min value) results to have more importance in the error determination than the others. In reality, this might not be due to the real importance of the feature itself, but simply to the numerical artifact. This might lead to suboptimal learning. In addition, the normalization improves the generalization performances of your model.\n",
    "\n",
    "There exist different types of normalizations. The most common are those that normalize the values in the range [0,1], and those that normalizes the data with respect to their standard deviation and mean (also called standardization). In this notebook, we use the standardization already implemented in the sklearn library.\n",
    "\n",
    "<b> Attention </b>: you can choose the normalization method you prefer, but you need to be careful to compute the normalization parameters (mean and standard deviation, for instance) only using the training data. Then, you re-use them to normalize also the testing set. It is important that the two are distinct, because if the testing set is included in the computation of the normalization parameters, then your test set will not be truly indipendent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae47e46d-bcb6-48dd-b1aa-49ef18b716e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the beginning of your input features now, to then compare it with the normalized ones\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f14bab-921a-444a-8c73-55aa58437f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalize input features (NOT THE TARGET, THAT WILL HAPPEN LATER)\n",
    "inpscaler = StandardScaler() # call the standardizer\n",
    "X_train = inpscaler.fit_transform(X_train) # this is the command that computes the parameters from the training set (fit) and then \n",
    "# already applies them for the normalization (transform)\n",
    "X_test = inpscaler.transform(X_test) # this is the part that transforms the test data based on the training parameters. Be careful, there is no fitting here.\n",
    "\n",
    "# print again the training set to see the difference\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a0806-9753-4522-84b3-5ba35359883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df5e5c-0613-4537-a8ee-c31207e7ce4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we first reshape the target, to make it 2Dimensional (required by the scaler)\n",
    "y_train = np.reshape(y_train,[-1,1])\n",
    "y_test = np.reshape(y_test,[-1,1])\n",
    "# Normalize the target\n",
    "tarscaler = StandardScaler() # call the standardizer\n",
    "y_train = tarscaler.fit_transform(y_train) # this is the command that computes the parameters from the training set (fit) and then \n",
    "# already applies them for the normalization (transform)\n",
    "y_test = tarscaler.transform(y_test) # this is the part that transforms the test data based on the training parameters. Be careful, there is no fitting here.\n",
    "\n",
    "# print again the training set to see the difference\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1cec68-6df4-45cb-9d4e-53cbfdfbe942",
   "metadata": {},
   "source": [
    "Why did we keep the target separated from the input features?\n",
    "Imagine that you have trained your model, which accepts as inputs normalized values and provides as outputs normalized values as well. Once you test the model, you will also obtain normalized values. You can then go back to the original dimensions by inverting the normalization. This step can be easily done in sklearn with a command, but for practical reasons it is easier to have one scaler only for the target. In case we used the same, indeed, we would have to input to the inverse normalization, one only matrix of input and output together (you can see that the dimensions of input and output are different)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1061df-dcfb-416c-88f2-66e6846134ab",
   "metadata": {},
   "source": [
    "# 6. Training and Testing the Multi Layer Perceptron (MLP) Model\n",
    "We will now build and train an MLP model. From a programming perspective, we could use two libraries to implement an MLP: scikit-learn (abbreviated sklearn) or tensorflow. The first is easier to handle as a first time hands on experience, but it is also less flexible and it is less suitable in case you have (very) large datasets. The second, instead, is more flexible and allows for better customization of the model, it is therefore suitable for Deep Learning models, but it is also less easy to handle as a first timer. We will use sklearn for our introduction.\n",
    "You can decide how many hidden layers to add and how many nodes each layer should have. This is regulated by the \"hidden_layer_sizes=(64,64)\". Currently, we have two hidden layers, both with 64 units each. You can change these numbers, add and reducing layers simply by adding/removing numbers. Mind to separate them with a comma. We also choose relu as activation function and adam optimizer (for the parameters optimization). The random state is a variable that controls that the initial weights are always the same, for the reproducibility of the notebook (you will not have changing results if you run the same model many times, unless you change parameters). You can learn more about the MLP regressor here https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec257e-03ed-47af-af74-5337af895ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we reshape back y_train and y_test, as MLP needs them 1D\n",
    "y_train = np.reshape(y_train,(y_train.shape[0]))\n",
    "y_test = np.reshape(y_test,(y_test.shape[0]))\n",
    "# Train MLP Model\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(64, 64), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715030cb-0a0b-4cb5-b283-8a3776e2a8da",
   "metadata": {},
   "source": [
    "Now we use the fitted model to predict the Qt+1 on the test set and we compute some metrics. The sklearn library does not have a built in method to track the loss function values during training. You can see it by applying the model on the training data and then computing the metrics. We show only the case for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787d88f-f41d-4660-91db-c098347a15bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Test MAE: {test_mae:.3f}\")\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {test_mse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d6b61-60a6-4315-9ed2-16d6da341ef4",
   "metadata": {},
   "source": [
    "There are several metrics that you can compute. You can look into the sklearn documentation which ones are already built in for you to use.\n",
    "\n",
    "It seems that the error is very low, but be careful: it is computed on the normalized dataset! We need to now <b>de-normalize</b> again our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87549e44-4736-43e2-894d-62c4d448162c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = mlp_model.predict(X_test).reshape(-1, 1)\n",
    "y_pred_inv = tarscaler.inverse_transform(y_pred)\n",
    "y_test_inv = tarscaler.inverse_transform(y_test.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212a377-0363-4919-bc74-75a0da074532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "print(f\"Test MAE: {test_mae:.3f}\")\n",
    "test_mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
    "print(f\"Test MSE: {test_mse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b3829-525a-4723-a615-a4882c3f2232",
   "metadata": {},
   "source": [
    "As you can see, the results change quite a lot! Remember that both MAE and MSE have dimensions (the same of the target)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c4e29-363c-4eb6-a498-82b4e64947ed",
   "metadata": {},
   "source": [
    "We can now plot our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e10ee-e8b6-4753-a2be-0bdce851d353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot actual vs predicted discharge\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test_inv, y_pred_inv, color='darkolivegreen', alpha=0.6, label='Predicted vs Actual')\n",
    "plt.plot([min(y_test_inv), max(y_test_inv)], [min(y_test_inv), max(y_test_inv)], color='black', linestyle='--', label='Perfect Fit')\n",
    "plt.xlabel('Actual Discharge (m³/s)')\n",
    "plt.ylabel('Predicted Discharge (m³/s)')\n",
    "plt.title('Multi Layer Perceptron: Predicted vs Actual Discharge')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "scatter_path = os.path.join(base_path, 'MLP_Scatter.png')\n",
    "# save the plot on your local folder\n",
    "plt.savefig(scatter_path, dpi=600, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03867c20-ccb5-4a51-baff-578a8c0171ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract the dates of the test set\n",
    "time = df['Date'].loc[:299]\n",
    "# Plot hydrographs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time,y_test_inv, label='Actual Discharge', color='navy')\n",
    "plt.plot(time,y_pred_inv, label='Predicted Discharge', color='steelblue', linestyle='dashed')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Discharge (m³/s)')\n",
    "plt.title('Actual vs Predicted Discharge Hydrograph')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "hydrograph_path = os.path.join(base_path, 'MLP_Hydrograph.png')\n",
    "# save the plot on your local folder\n",
    "plt.savefig(hydrograph_path, dpi=600, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6eee57-bb7d-46c0-9e0e-54187bfcac25",
   "metadata": {},
   "source": [
    "<b> We can now save the results locally, on our laptop </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b6856-37b9-4ae4-b95f-c7c1e491f283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First we create a dataframe with the observations and the predictions\n",
    "dr = pd.DataFrame(columns=['Date', 'Obs', 'MLP'])\n",
    "dr['MLP'] = pd.DataFrame(y_pred_inv)\n",
    "dr['Obs'] = pd.DataFrame(y_test_inv)\n",
    "dr['Date'] = pd.to_datetime(time)\n",
    "dr.head()\n",
    "\n",
    "# Now we save them\n",
    "results_path = os.path.join(base_path, 'MLP_Predictions.xlsx')\n",
    "dr.to_excel(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b3461-627f-4075-84d4-4ab12466bac5",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "We have learnt how to split the training and testing data using pandas library. We then learnt how and why to normalize input features, how to call, train and test a MLP model. Finally, we have checked the performances using the mean squared error and with graphical inspections. What can you conclude about the different models tested? Which would you choose as the best model?\n",
    "<b> Which model is the best among all those we implemented? Do you see much difference between them? </b>\n",
    "\n",
    "You can keep exploring different training strategies, adding/reducing hidden layers and/or nodes, but also changing the activation function. Always monitor what happens to your metrics when changing parameters!\n",
    "\n",
    "If you want, you can save all your predictions externally and then plot them all toghether again. You only need to load the saved data and adjust the plotting codes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
